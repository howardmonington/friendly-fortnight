{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3022: Intro to Data Science - Fall 2018 Practicum \n",
    "***\n",
    "\n",
    "This practicum is due on Moodle by **11:55pm on Wednesday December 12**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  \n",
    "\n",
    "**Here are the rules:** \n",
    "\n",
    "1. All work, code and analysis, must be your own. \n",
    "1. You may use your course notes, posted lecture slides, textbooks, in-class notebooks, and homework solutions as resources.  You may also search online for answers to general knowledge questions like the form of a probability distribution function or how to perform a particular operation in Python/Pandas. \n",
    "1. This is meant to be like a coding portion of your final exam. So, the instructional team will be much less helpful than we typically are with homework. For example, we will not check answers, help debug your code, and so on.\n",
    "1. If something is left open-ended, it is because we want to see how you approach the kinds of problems you will encounter in the wild, where it will not always be clear what sort of tests/methods should be applied. Feel free to ask clarifying questions though.\n",
    "2. You may **NOT** post to message boards or other online resources asking for help.\n",
    "3. You may **NOT** copy-paste solutions *from anywhere*.\n",
    "4. You may **NOT** collaborate with classmates or anyone else.\n",
    "5. In short, **your work must be your own**. It really is that simple.\n",
    "\n",
    "Violation of the above rules will result in an immediate academic sanction (*at the very least*, you will receive a 0 on the practicum or an F in the course, depending on severity), and a trip to the Honor Code Council.\n",
    "\n",
    "**By submitting this assignment, you agree to abide by the rules given above.**\n",
    "\n",
    "***\n",
    "\n",
    "**Name**:  \n",
    "Alex Kennedy\n",
    "***\n",
    "\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- You may not use late days on the practicum nor can you drop your practicum grade. \n",
    "- If you have a question for us, post it as a **PRIVATE** message on Piazza.  If we decide that the question is appropriate for the entire class, then we will add it to a Practicum clarifications thread. \n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Problem 3](#p3)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np \n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p1'></a>\n",
    "\n",
    "### [35 points] Problem 1: Yahtzee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A:** You are playing [Yahtzee](https://en.wikipedia.org/wiki/Yahtzee) with your friends. A player's turn in Yahtzee consists of rolling a set of 5 dice. Then the player is given two additional rolls, where they are allowed to re-roll any number of the dice, including potentially all of them or none of them. The goal is to obtain certain combinations of the dice values resulting after the third roll. Different combinations are worth different amounts of points, and the goal of the game is to get as many points as possible.\n",
    "\n",
    "This game of Yahtzee is a bit unlike any you have ever played before, however. This is because Darth Ketelsen is back, and with her she brought her famous **5-sided dice**. These are fair dice with sides numbered 1-5. So, you are playing Yahtzee with a Sith Lord with 5-sided dice. Indeed, things just got real.\n",
    "\n",
    "A **straight** in Darth Ketelsen's game consists of 5 values all in a row. For example, the outcome $[1,2,3,4,5]$ is a  straight but the outcome $[1,2,3,4,4]$ is not.\n",
    "\n",
    "**Do two things:**\n",
    "1. Compute by hand the probability of rolling a straight in a single roll of all 5 dice. Show all work.\n",
    "2. Write a simulation to verify the probability that you computed. Run at least 10,000 simulations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Part A:__\n",
    "\n",
    "The probability of rolling a straight in a single roll of all dice is the probability of rolling 5 distinct numbers.  For this to be possible you must roll 5 options for the first die, 4 for the second (removing the one you already rolled), then 3 options, etc. This means there are 5! = 120 ways to roll a straight.\n",
    "\n",
    "Total, there are $ 5^5 $ possible ways to roll the dice.\n",
    "\n",
    "This means there is a $\\frac{5!}{5^5}$ chance of rolling a straight in one roll.\n",
    "\n",
    "$ \\frac{5!}{5^5} = 0.0384$\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0384\n",
      "Calculated probability: 0.0384\n",
      "Simulated probability: 0.03903\n"
     ]
    }
   ],
   "source": [
    "# randomly roll 5 dice\n",
    "def roll5dice():\n",
    "    dice = []\n",
    "    for _ in range(5):\n",
    "        dice.append(random.randint(1,5))\n",
    "    return dice\n",
    "\n",
    "# checks if the dice are rolled in a straight\n",
    "def isStraight(dice):\n",
    "    s = set()\n",
    "    for die in dice:\n",
    "        if die in s:\n",
    "            return False\n",
    "        else: \n",
    "            s.add(die)\n",
    "    return True\n",
    "\n",
    "print(120/5**5)\n",
    "\n",
    "# calculate the probability of rolling a straight with a single roll turn\n",
    "def prob_straight_single_roll(num_sims):\n",
    "    straights = 0\n",
    "    for _ in range(num_sims):\n",
    "        if(isStraight(roll5dice())):\n",
    "            straights += 1\n",
    "    return straights / num_sims\n",
    "        \n",
    "print(f\"Calculated probability: {120/5**5}\")\n",
    "print(f\"Simulated probability: {prob_straight_single_roll(100000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B:** The goal of this problem is to compute the probability of getting a straight using all three of your rolls, instead of just the single roll approach that you computed in Part A. Here, we'll need to implement a strategy so that after the first roll and after the second roll, we keep the dice that get us closer to a straight and re-roll the dice that are not useful for our straight.\n",
    "\n",
    "For instance, suppose your first roll comes up $[1,2,3,3,3]$. You really want to get that straight! So, you would follow the strategy of saving the $[1,2,3]$ and re-roll two of the threes, hoping for a 4 and 5 to get the straight. Then, for your third roll, you would save as many of the dice as possible that would be part of a straight, and re-roll any remaining dice.\n",
    "\n",
    "Finish the function below called `dire_straights` to simulate many complete 3-roll turns, and computes the probability of ending your turn with a straight. The only input to the function should be `ntrial`, an integer for the number of turns to simulate. Remember, each turn consists of 3 rolls.\n",
    "\n",
    "\n",
    "Then, use your function to estimate the probability of a straight after a full turn of Yahtzee. Use at least 10,000 simulations, and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of getting a straight with strategy: 0.2004\n"
     ]
    }
   ],
   "source": [
    "# build a binary array deciding which dice to keep (the ones that build a straight)\n",
    "def getkeepersstraight(dice):\n",
    "    keepers = [0,0,0,0,0]\n",
    "    # check for two adjacent\n",
    "    num_dice = len(dice)\n",
    "    foundAdjacent = False\n",
    "    min_die, max_die = 0,0\n",
    "    for i in range(num_dice):\n",
    "        if foundAdjacent:\n",
    "            break\n",
    "        for j in range(i + 1, num_dice):\n",
    "            if foundAdjacent:\n",
    "                break\n",
    "            dice_difference = dice[i] - dice[j]\n",
    "            if abs(dice_difference) == 1:\n",
    "                keepers[i] = 1\n",
    "                keepers[j] = 1\n",
    "                if dice[i] < dice[j]:\n",
    "                    min_die = dice[i]\n",
    "                    max_die = dice[j]\n",
    "                else:\n",
    "                    min_die = dice[j]\n",
    "                    max_die = dice[i]\n",
    "                    \n",
    "                foundAdjacent = True\n",
    "    # add on to adjacent at the ends\n",
    "    for i in range(len(dice)):\n",
    "        if dice[i] - min_die == -1:\n",
    "            min_die = dice[i]\n",
    "            keepers[i] = 1\n",
    "        elif dice[i] - max_die == 1:\n",
    "            max_die = dice[i]\n",
    "            keepers[i] = 1\n",
    "    return keepers\n",
    "\n",
    "# reroll the ones that you aren't keeping\n",
    "def reroll(dice_set, keepers):\n",
    "    newdice = []\n",
    "    for i in range(len(dice_set)):\n",
    "        if keepers[i] == 0:\n",
    "            newdice.append(random.randint(1,5))\n",
    "        else:\n",
    "            newdice.append(dice_set[i])\n",
    "    return newdice\n",
    "\n",
    "def dire_straights(ntrial):\n",
    "    num_straights = 0\n",
    "    for trial in range(ntrial):\n",
    "        foundstraight = False\n",
    "        turn = 1\n",
    "        dice = roll5dice()\n",
    "        # roll 3 times OR until you get a straight\n",
    "        for turn in range(2):\n",
    "            if isStraight(dice):\n",
    "                num_straights += 1\n",
    "                foundstraight = True\n",
    "                break\n",
    "            dice = reroll(dice, getkeepersstraight(dice))\n",
    "        if isStraight(dice) and not foundstraight:\n",
    "            num_straights += 1\n",
    "    return num_straights / ntrial # this is a placeholder\n",
    "\n",
    "print(f\"Probability of getting a straight with strategy: {dire_straights(20000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "__Part B Conclusions:__\n",
    "\n",
    "The probability of getting a straight on a 3 roll turn is consistently around 20% with the three roll turns on 20k simulations.  Another idea for this strategy would be to save all distinct values. In theory this would give you a higher probability because a straight is built up of all distinct values.  This constrasts the strategy above because in a roll of [1,2,3,3,5] one would save 1, 2, and 3 instead of 1, 2, 3, and 5.  Saving the second set would get you closer to the straight than the first one.  This theory will be tested with the different strategy in part C.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C:** Write a simulation to estimate the probability of obtaining a straight if the first roll contains exactly three distinct unique values. For example, a valid first roll could be $[1,5,3,3,3]$ but not $[1,3,3,4,5]$. You are still using the set of 5-sided dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated probability of getting a straight given 3 distinct starter dice 0.224\n"
     ]
    }
   ],
   "source": [
    "def getdistinct(dice):\n",
    "    keptvalues = set()\n",
    "    keepers = [0,0,0,0,0]\n",
    "    for die_index in range(len(dice)):\n",
    "        if dice[die_index] not in keptvalues:\n",
    "            keptvalues.add(dice[die_index])\n",
    "            keepers[die_index] = 1\n",
    "    return keepers\n",
    "\n",
    "def distinct_straight_sim(ntrials):\n",
    "    num_straights = 0\n",
    "    for trial in range(ntrials):\n",
    "        # start with the condition which is 3 distinct dice exactly\n",
    "        dice = [1,2,3,3,3]\n",
    "        # play the rest of the two turns to see if you can pull a straight\n",
    "        for turn in range(2):\n",
    "            # re-roll the dice that aren't distinct\n",
    "            dice = reroll(dice, getdistinct(dice))\n",
    "            # test for straight\n",
    "            if isStraight(dice):\n",
    "                num_straights += 1\n",
    "                break\n",
    "    # return the probability of getting a straight in the trials\n",
    "    # return num_straights / ntrials\n",
    "    return num_straights / ntrials\n",
    "\n",
    "print(f\"Simulated probability of getting a straight given 3 distinct starter dice {round(distinct_straight_sim(20000), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D:** Verify your calculation from Part C by hand. Show all work, and comment on whether the two agree.\n",
    "\n",
    "*Hint: you will need to consider a variety of different cases - what are all the ways you could end up with a straight, given that your first roll contained exactly 3 unique values?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Part D:__\n",
    "\n",
    "We are given that the first roll contains 3 distinct numbers on the dice.  This means that there are only a few options left of ways that we can get a straight.  Using the law of total probability we can calculate the total probability of rolling a straight given this starting condition.\n",
    "\n",
    "Ways to get a straight:\n",
    "\n",
    "* Roll 0 on the second turn, and 2 on the third turn.  This means rolling 2 repeats of numbers that you saved on your second turn, and then finishing the straight on the 3rd turn.  The probability of this happening is $\\frac{2}{25}$. $\\frac{2}{5}$ chance to roll 1 new die AND $\\frac{1}{5}$ to finish the straight. AND rolling 0 right on the second turn which is $\\frac{9}{25}$\n",
    "\n",
    "* Roll 1 right on the second turn and 2 on the third turn.  The probability of this event is $\\frac{14}{25} * \\frac{1}{5}$.  Rolling 1 right and 1 repeat on the second turn has a probability of $\\frac{14}{25}$ as one can see from a chart that displays all possible rolls.  1-5 on the x and 1-5 on the y, there are 14 boxes out of the 25 that fulfill this requirement. On the third roll you have a 1/5 chance to get the finishing blow for the straight.\n",
    "\n",
    "* Roll 2 right on the second turn, and the straight is finished there.  The probability of this is the same as the third roll of the first way to get a straight: $\\frac{2}{25}$\n",
    "\n",
    "If you use the law of total probability on all of these values the final calculated result is\n",
    "\n",
    "$$\n",
    "    \\frac{9}{25} * \\frac{2}{25} + \\frac{14}{25} * \\frac{1}{5} + \\frac{2}{25} = 0.2208\n",
    "$$\n",
    "\n",
    "This result agrees with the simulated value\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E:**  \n",
    "Your friend offers you the following deal. Each time your Yahtzee turn (i.e., all three rolls) results in a 5-of-a-kind, she will give you \\$5.\n",
    "Each time your Yahtzee turn results in a straight, she will give you \\$3.\n",
    "But, she will charge you \\$1 for each turn (where a turn includes all 3 rolls of the five 5-sided dice). Should you take this deal? Fully justify your answer using calculations that include expected values. You may include some simulations to estimate relevant probabilities. Clearly state any assumptions you are making in your modeling choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Part E:__\n",
    "\n",
    "2 strategies, going for the straight or going for the 5-of-a-kind. Decide the strategy based on the first roll.\n",
    "\n",
    "1st strategy, going for the straight.\n",
    "Save the distinct dice.  First, calculuate the probability of getting a straight depending on how many distinct dice are in the start using a simulation.\n",
    "\n",
    "2nd strategy, going for the five of a kind.\n",
    "\n",
    "This can be shortcut by just finding the expected values for both of the strategies.  The expected value for each of the strategies can be broken into arrays of expected win values for each of the different types of starts for the strategies.  The starts are scored based on the number of dice that put you closer toward the goal.  In the case of the Yahtzee strategy, the index in the array is the number of dice that you started with that are the same as one another.\n",
    "\n",
    "In the case of the straight strategy, the index in the array is the number of distinct values that were rolled on the first roll.\n",
    "\n",
    "Both of these arrays can be multiplied with their counterpart that is the probability of starting with this number of 'points on the first roll', and then each value can be multiplied with the win amount to get the expected value for each start.\n",
    "\n",
    "That is, E(X) = prob(starting points) * win_amount.\n",
    "\n",
    "With these two expected arrays printed we can see that for each possible starting combination of dice, the expected value is ALWAYS less than 1$, which is what we paid for the game. Thus, we should never play this game\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected win for yahtzee strat: [0.005250000000000001, 0.15955, 0.1672, 0.06135, 0.007200000000000001]\n",
      "expected win for straight strat: [0.00075, 0.04698, 0.31314, 0.41193, 0.11745]\n"
     ]
    }
   ],
   "source": [
    "def distinct_straight_sim_return_array(ntrials):\n",
    "    num_straights = [0,0,0,0,0]\n",
    "    starts = [0,0,0,0,0]\n",
    "    for trial in range(ntrials):\n",
    "        foundStraight = False\n",
    "        # start with the condition which is 3 distinct dice exactly\n",
    "        dice = roll5dice()\n",
    "        num_distinct_start = sum(getdistinct(dice))\n",
    "        if isStraight(dice):\n",
    "                num_straights[num_distinct_start - 1] += 1\n",
    "                foundStraight = True\n",
    "        starts[num_distinct_start - 1] += 1\n",
    "        # play the rest of the two turns to see if you can pull a straight\n",
    "        for turn in range(2):\n",
    "            if foundStraight: \n",
    "                break\n",
    "            # re-roll the dice that aren't distinct\n",
    "            dice = reroll(dice, getdistinct(dice))\n",
    "            # test for straight\n",
    "            if isStraight(dice):\n",
    "                num_straights[num_distinct_start - 1] += 1\n",
    "                break\n",
    "    # probs of getting the straight give that you rolled a first roll with number of distinct dice equal to the index (zero based) + 1\n",
    "    prob_win = []\n",
    "    for i in range(len(num_straights)):\n",
    "        if(starts[i] != 0):\n",
    "            prob_win.append(num_straights[i] / starts[i])\n",
    "        else:\n",
    "            prob_win.append(0)\n",
    "    prob_roll_start = [x / ntrials for x in starts]\n",
    "    # return the probability of getting a straight in the trials\n",
    "    return prob_win, prob_roll_start\n",
    "\n",
    "# return the keepers going for a five of a kind strategy, keeping the number that you have the most of\n",
    "def get_most_same_keepers(dice):\n",
    "    keepers = []\n",
    "    num_with_most = 0\n",
    "    most_found = 0\n",
    "    for i in range(len(dice)):\n",
    "        num_current = 1\n",
    "        for j in range(len(dice)):\n",
    "            if dice[i] == dice[j] and i != j:\n",
    "                num_current += 1\n",
    "        if num_current > most_found:\n",
    "            most_found = num_current\n",
    "            num_with_most = dice[i]\n",
    "    \n",
    "    for die in dice:\n",
    "        if die == num_with_most:\n",
    "            keepers.append(1)\n",
    "        else:\n",
    "            keepers.append(0)\n",
    "    return keepers\n",
    "\n",
    "def fiveofakind_sim_return_array(ntrials):\n",
    "    num_fives = [0,0,0,0,0]\n",
    "    starts = [0,0,0,0,0]\n",
    "    for trial in range(ntrials):\n",
    "        foundYahtzee = False\n",
    "        dice = roll5dice()\n",
    "        num_started = sum(get_most_same_keepers(dice))\n",
    "        if sum(get_most_same_keepers(dice)) == 5:\n",
    "            num_fives[num_started - 1] += 1\n",
    "            foundYahtzee = True\n",
    "        starts[num_started - 1] += 1\n",
    "        for turn in range(2):\n",
    "            if foundYahtzee == True:\n",
    "                break\n",
    "            dice = reroll(dice, get_most_same_keepers(dice))\n",
    "            if sum(get_most_same_keepers(dice)) == 5:\n",
    "                num_fives[num_started - 1] += 1\n",
    "                break\n",
    "    prob_win = []\n",
    "    for i in range(len(num_fives)):\n",
    "        if(starts[i] != 0):\n",
    "            prob_win.append(num_fives[i] / starts[i])\n",
    "        else:\n",
    "            prob_win.append(0)\n",
    "    prob_roll_start = [x / ntrials for x in starts]\n",
    "    return prob_win, prob_roll_start\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "straight_win = 3\n",
    "prob_win_straight, prob_straight_start = distinct_straight_sim_return_array(100000)\n",
    "prob_win_five, prob_five_start = fiveofakind_sim_return_array(100000)\n",
    "expected_five = []\n",
    "for i in range(5):\n",
    "    expected_five.append(prob_five_start[i] * prob_win_five[i] * 5)\n",
    "print(f\"expected win for yahtzee strat: {expected_five}\")\n",
    "expected_straight = []\n",
    "for i in range(5):\n",
    "    expected_straight.append(prob_straight_start[i] * prob_win_straight[i] * 3)\n",
    "print(f\"expected win for straight strat: {expected_straight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "### [30 points] Problem 2: Sharknado Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Governor Hickenlooper has charged you with the task of assessing the factors associated with sharknado risk in Colorado. As everyone knows, sharknadoes are a leading cause of sharknado-related illness, and you are a world-renowned data/shark scientist.\n",
    "\n",
    "You decide to use multiple linear regression to understand and predict what factors lead to increased sharknado hazard. Your lead scientist, aptly named Fin, has collected lots of relevant data at a local sharknado hotspot, the Boulder Reservoir[\\*](#footnote). The data cover a variety of sharknado-related environmental and other conditions, and you'll find this data in the file `sharknadoes.csv`. \n",
    "\n",
    "**Response**: \n",
    "\n",
    "- $\\texttt{sharknado hazard}$: the hazard of a sharknado, where 1 is very unlikely and 100 is highly likely\n",
    "\n",
    "**Features**: \n",
    "\n",
    "- $\\texttt{taunts}$: the number of times over the past year that someone has taunted a shark\n",
    "- $\\texttt{clouds}$: what percentage of the sky was covered by clouds (fraction, 0-1)\n",
    "- $\\texttt{precipitation}$: amount of precipitation in the past 72 hours (inches)\n",
    "- $\\texttt{earthquake}$: the intensity of the most recent earthquake measured in the continental United States\n",
    "- $\\texttt{shark attacks}$: the number of shark attacks within 72 hours prior to the observation\n",
    "- $\\texttt{ice cream sold}$: the number of units of ice cream sold at the beach concession stand \n",
    "- $\\texttt{misery index}$: an economic indicator for how miserable the average United States citizen is, based on the unemployment rate and the inflation rate. More [here](https://www.stuffyoushouldknow.com/podcasts/whats-the-misery-index.htm) and [here](https://en.wikipedia.org/wiki/Misery_index_(economics)). Higher values correspond to more miserable citizens.\n",
    "- $\\texttt{temperature}$: the outside temperature, measured in degrees Fahrenheit\n",
    "- $\\texttt{humidity}$: relative humidity (percent, 0-100)\n",
    "- $\\texttt{pizzas sold}$: the number of pizzas sold at the beach concession stand in the past year\n",
    "- $\\texttt{pressure}$: local air pressure (millibar) \n",
    "- $\\texttt{octopuses}$: the number of octupuses in the vicinity on the day of the observation\n",
    "- $\\texttt{Dan's shoe size}$: the size of the shoes Dan was wearing when the observation was made\n",
    "- $\\texttt{Tony's shoe size}$: the size of the shoes Tony was wearing when the observation was made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Read the data from `sharknadoes.csv` into a Pandas DataFrame.  Note that since we will be doing a multiple linear regression we will need all of the features, so you should drop any row in the DataFrame that is missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   clouds  earthquake  pizzas sold  taunts  pressure  shark attacks  \\\n",
      "0    1.00         7.1       5560.0    15.0    847.12            2.0   \n",
      "1    1.00         7.4       5179.0    20.0    844.34            4.0   \n",
      "2    1.00         7.0       5227.0     0.0    839.48            9.0   \n",
      "3    0.13         7.9       5226.0    34.0    851.28            2.0   \n",
      "4    1.00         7.5       5491.0     6.0    852.67            2.0   \n",
      "\n",
      "   octopuses  precipitation  misery index  ice cream sold  humidity  \\\n",
      "0        7.0       0.824059     12.987180           273.0     86.41   \n",
      "1        5.0       0.993296     16.765435           184.0     96.67   \n",
      "2        2.0       1.173342     16.494518           141.0     53.85   \n",
      "3        6.0       0.919291      8.277176           146.0     88.72   \n",
      "4        4.0       1.729127      5.904750           178.0     63.08   \n",
      "\n",
      "   temperature  Dans shoe size  Tonys shoe size  sharknado hazard  \n",
      "0         78.0            42.0              9.0             40.22  \n",
      "1         89.0            42.0              9.5             36.42  \n",
      "2         65.0             9.5              9.0             19.54  \n",
      "3         36.0             9.5             10.0             85.00  \n",
      "4         72.0            42.0              9.0             56.34  \n"
     ]
    }
   ],
   "source": [
    "dfSn = pd.read_csv('data/sharknadoes.csv').dropna()\n",
    "print(dfSn.head())\n",
    "dfFull = dfSn.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Perform the appropriate statistical test at the $\\alpha = 0.01$ significance level to determine if _at least one_ of the features is related to the the response $y$.  Clearly describe your methodology and show all computations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Part B Work:__\n",
    "\n",
    "Test for overall significance, using F-test.\n",
    "$$\n",
    "    H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\dots = 0 \\\\\n",
    "    H_1: \\text{at least 1 slope coeff}\\neq0\n",
    "$$\n",
    "\n",
    "Computing the F-statistic\n",
    "$$\n",
    "    F = \\frac{\\frac{SST - SSE}{p}}{\\frac{SSE}{n-p-1}} \\\\\n",
    "    \\bar{y} = 60.101 \\\\\n",
    "    n = 72 \\\\\n",
    "    p = 14 \\\\\n",
    "    \\hat{y_i} = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\beta_3x_{3i} + \\dots\\\\\n",
    "    SST = \\sum_{i=1}^n{(y_i - \\bar{y})^2} = 24010.692\\\\\n",
    "    SSE = \\sum_{i=1}^n{(y_i - \\hat{y}_i)^2} = 532.944\\\\\n",
    "    F = 179.4 \\\\ \n",
    "    \\text{F-critical}_{13, 54} = \\text{from table or python} = 2.479\\\\\n",
    "$$\n",
    "\n",
    "Because F > F-critical we can reject the null hypothesis and assert that at least one feature is related to the response, sharknado hazard.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dfSn.columns.values[:-1]\n",
    "# collect features in 2D array\n",
    "X = dfSn[features]\n",
    "\n",
    "# add a constant to the array for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# collect the response data in the array\n",
    "Y = dfSn['sharknado hazard']\n",
    "\n",
    "model = sm.OLS(Y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       sharknado hazard   R-squared:                       0.978\n",
      "Model:                            OLS   Adj. R-squared:                  0.972\n",
      "Method:                 Least Squares   F-statistic:                     179.4\n",
      "Date:                Wed, 12 Dec 2018   Prob (F-statistic):           9.60e-42\n",
      "Time:                        13:48:23   Log-Likelihood:                -174.23\n",
      "No. Observations:                  72   AIC:                             378.5\n",
      "Df Residuals:                      57   BIC:                             412.6\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "const           -2549.8985     67.605    -37.718      0.000   -2685.275   -2414.522\n",
      "clouds             -1.5106      2.566     -0.589      0.558      -6.650       3.628\n",
      "earthquake          2.5079      0.467      5.367      0.000       1.572       3.444\n",
      "pizzas sold        -0.0006      0.002     -0.373      0.711      -0.004       0.003\n",
      "taunts              0.3117      0.042      7.447      0.000       0.228       0.396\n",
      "pressure            3.0688      0.079     38.850      0.000       2.911       3.227\n",
      "shark attacks      -0.1151      0.144     -0.797      0.429      -0.404       0.174\n",
      "octopuses          -0.0749      0.143     -0.524      0.602      -0.361       0.211\n",
      "precipitation       1.3982      0.930      1.503      0.138      -0.464       3.261\n",
      "misery index        0.0273      0.080      0.340      0.735      -0.133       0.188\n",
      "ice cream sold      0.0096      0.008      1.193      0.238      -0.007       0.026\n",
      "humidity            0.0188      0.027      0.706      0.483      -0.035       0.072\n",
      "temperature        -0.4426      0.053     -8.396      0.000      -0.548      -0.337\n",
      "Dans shoe size      0.0271      0.023      1.175      0.245      -0.019       0.073\n",
      "Tonys shoe size     0.2814      1.273      0.221      0.826      -2.268       2.831\n",
      "==============================================================================\n",
      "Omnibus:                        0.821   Durbin-Watson:                   2.225\n",
      "Prob(Omnibus):                  0.663   Jarque-Bera (JB):                0.300\n",
      "Skew:                           0.005   Prob(JB):                        0.861\n",
      "Kurtosis:                       3.316   Cond. No.                     1.03e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.03e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "SSE: 532.943670740439\n",
      "SST: 24010.692465277767\n",
      "F: 179.3584997493224\n",
      "F-critical: 2.478620068997522\n"
     ]
    }
   ],
   "source": [
    "# used for testing...\n",
    "print(model.summary())\n",
    "\n",
    "def sst(y):\n",
    "    sst = 0\n",
    "    ybar = y.mean()\n",
    "    for cur in y:\n",
    "        sst += (cur- ybar)**2\n",
    "    return sst\n",
    "\n",
    "def sse(x, y, model):\n",
    "    my_y = y.values\n",
    "    params = model.params.values\n",
    "    sse = 0\n",
    "    for i in range(len(my_y)):\n",
    "        x_num = x.iloc[i].values\n",
    "        yhat = params[0]\n",
    "        for j in range(1, len(x_num)):\n",
    "            yhat += params[j] * x_num[j]\n",
    "        sse += (my_y[i] - yhat) **2\n",
    "    return sse\n",
    "\n",
    "def compute_f_stat(X, Y, model, p, n):\n",
    "    mysse = sse(X, Y, model)\n",
    "    top = (sst(Y) - mysse)/p\n",
    "    bottom = mysse/(n - p - 1)\n",
    "    return top/bottom\n",
    "    \n",
    "p = 14\n",
    "n = 72\n",
    "f = compute_f_stat(X, Y, model, p, n)\n",
    "fcrit = scipy.stats.f.ppf(1-0.01, 13, 54)\n",
    "print(f\"SSE: {sse(X, Y, model)}\")\n",
    "print(f\"SST: {sst(Y)}\")\n",
    "print(f\"F: {f}\")\n",
    "print(f\"F-critical: {fcrit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Part C**: Write a function `backward_select(df, resp_str, maxsse)` that takes in the DataFrame (`df`), the name of the column corresponding to the response (`resp_str`), and the maximum desired sum of squared errors (`maxsse`), and returns a list of feature names corresponding to the most important features via backward selection.  Use your code to determine the reduced MLR model with the minimal number of features such that the SSE of the reduced model is less than 570. At each stage in backward selection you should remove the feature that has the highest p-value associated with the hypothesis test for the given slope coefficient $\\beta_k \\neq 0$.\n",
    "\n",
    "Your code should clearly indicate which feature was removed in each stage, and the SSE associated with the model fit before the feature's removal. _Specifically, please write your code to print the name of the feature that is going to be removed and the SSE before its removal_. Afterward, be sure to report all of the retained features and the SSE of the reduced model.\n",
    "\n",
    "**Note**: The point of this exercise is to see if you can implement **backward_select** yourself.  You may of course use canned routines like statmodels OLS, but you may not call any Python method that explicitly performs backward selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_select(df, resp_str, maxsse):\n",
    "    # make a copy of the data-frame\n",
    "    dropping_df = df.copy()\n",
    "    \n",
    "    # list of the remaining features that will be returned\n",
    "    remaining_features = []\n",
    "    \n",
    "    # get the initial values for SSE\n",
    "    features = df.columns.values[:-1]\n",
    "    # collect features in 2D array\n",
    "    X = df[features]\n",
    "    # add a constant to the array for the intercept\n",
    "    X = sm.add_constant(X)\n",
    "    # collect the response data in the array\n",
    "    Y = df['sharknado hazard']\n",
    "    # make the model\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    my_sse = sse(X, Y, model)\n",
    "    last_safe = dropping_df.copy()\n",
    "    \n",
    "    while my_sse < 570:\n",
    "        last_safe = dropping_df.copy()\n",
    "        # should delete tony's shoe size\n",
    "        pvals = model.pvalues\n",
    "        delete_column = list(pvals.nlargest(1).index)\n",
    "        print(f\"deleting: {delete_column[0]}, previous sse: {my_sse}\")\n",
    "\n",
    "        # remove this from the Df\n",
    "        dropping_df = dropping_df.drop(columns = delete_column)\n",
    "\n",
    "        features = dropping_df.columns.values[:-1]\n",
    "        # collect features in 2D array\n",
    "        X = dropping_df[features]\n",
    "        # add a constant to the array for the intercept\n",
    "        X = sm.add_constant(X)\n",
    "        # collect the response data in the array\n",
    "        Y = dropping_df['sharknado hazard']\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        my_sse = sse(X, Y, model)\n",
    "        if my_sse > 570:\n",
    "            print(f\"Oops! Deleting feature: {delete_column[0]}, caused SSE to go over specified max. Restoring this column\")\n",
    "        \n",
    "    remaining_features = last_safe.columns.values[:-1]\n",
    "    \n",
    "    return remaining_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Write down the multiple linear regression model, including estimated parameters, obtained by your backward selection process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting: Tonys shoe size, previous sse: 532.943670740439\n",
      "deleting: misery index, previous sse: 533.4003410558859\n",
      "deleting: octopuses, previous sse: 534.6875698945762\n",
      "deleting: clouds, previous sse: 536.5069911256511\n",
      "deleting: pizzas sold, previous sse: 539.4965616357625\n",
      "deleting: humidity, previous sse: 543.3572004721018\n",
      "deleting: shark attacks, previous sse: 547.4141982866453\n",
      "deleting: Dans shoe size, previous sse: 552.1548653819108\n",
      "deleting: ice cream sold, previous sse: 564.1068895239317\n",
      "Oops! Deleting feature: ice cream sold, caused SSE to go over specified max. Restoring this column\n",
      "const            -2542.039198\n",
      "earthquake           2.401187\n",
      "taunts               0.318776\n",
      "pressure             3.061834\n",
      "precipitation        1.255909\n",
      "ice cream sold       0.009931\n",
      "temperature         -0.468043\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akenn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "selected_features = backward_select(dfSn, 'sharknado hazard', 570)\n",
    "dfSelect = dfSn[selected_features]\n",
    "dfSelect['sharknado hazard'] = dfSn['sharknado hazard']\n",
    "\n",
    "features = dfSelect.columns.values[:-1]\n",
    "# collect features in 2D array\n",
    "X = dfSelect[features]\n",
    "# add a constant to the array for the intercept\n",
    "X = sm.add_constant(X)\n",
    "# collect the response data in the array\n",
    "Y = dfSelect['sharknado hazard']\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Part D: Multiple regression model__\n",
    "\n",
    "\n",
    "Beta values printed in the cell above\n",
    "$$\n",
    "    y = -2542.04 + 2.40*earthquake_i + 0.32*taunts_i + 3.06 * pressure_i + 1.26 * precipitation_i + 0.01 * icecreamsold_i + -0.47 * temperature_i\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Perform the appropriate statistical test at the $\\alpha = 0.01$ significance level to determine whether there is a statistically significant difference between the full model with all features and the reduced model obtained by backward selection in **Part D**. You may use output from your model fit above, but all calculations should be set up in Markdown/MathJax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Part E:__\n",
    "\n",
    "To determine if there is a statistically significant difference, we use an F test like the following and compare to a critical F value\n",
    "\n",
    "$$\n",
    "    F = \\frac{\\frac{SSE_{red} - SSE_{full}}{p-k}}{\\frac{SSE_{full}}{n-p-1}} = 0.417 \\\\\n",
    "    F_{p-k, n-p-1} = 0.198\n",
    "$$\n",
    "\n",
    "Because our F-stat is greater than the F-critical from the table/python we can determine that the reduced model has a better fit.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f-stat: 0.4166255197737881\n",
      "f-crit: 0.19839178421766424\n"
     ]
    }
   ],
   "source": [
    "featuresfull = dfFull.columns.values[:-1]\n",
    "# collect features in 2D array\n",
    "Xfull = dfFull[featuresfull]\n",
    "\n",
    "# add a constant to the array for the intercept\n",
    "Xfull = sm.add_constant(Xfull)\n",
    "\n",
    "# collect the response data in the array\n",
    "Yfull = dfFull['sharknado hazard']\n",
    "\n",
    "modelfull = sm.OLS(Yfull, Xfull).fit()\n",
    "\n",
    "featuresred = dfSelect.columns.values[:-1]\n",
    "# collect features in 2D array\n",
    "Xred = dfSelect[featuresred]\n",
    "# add a constant to the array for the intercept\n",
    "Xred = sm.add_constant(Xred)\n",
    "# collect the response data in the array\n",
    "Yred = dfSelect['sharknado hazard']\n",
    "modelred = sm.OLS(Yred, Xred).fit()\n",
    "\n",
    "topdof = len(featuresfull) - len(featuresred)\n",
    "bottomdof = len(dfSn) - len(featuresfull) - 1\n",
    "\n",
    "top = ((sse(Xred, Yred, modelred) - sse(Xfull, Yfull, modelfull)) / (topdof))\n",
    "bottom = sse(Xfull, Yfull, modelfull) / (bottomdof)\n",
    "F = top/bottom\n",
    "fcrit = scipy.stats.f.ppf(0.01, topdof, bottomdof)\n",
    "print(f\"f-stat: {F}\\nf-crit: {fcrit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Based on your conclusions in **Part E**, use the _better_ of the two models to predict the sharknado hazard when the following features are observed: \n",
    "\n",
    "- $\\texttt{taunts}$: 47\n",
    "- $\\texttt{clouds}$: 0.8\n",
    "- $\\texttt{precipitation}$: 1 inch\n",
    "- $\\texttt{earthquake}$: 5\n",
    "- $\\texttt{shark attacks}$: 11\n",
    "- $\\texttt{ice cream sold}$: 120\n",
    "- $\\texttt{misery index}$: 15\n",
    "- $\\texttt{temperature}$: 70 degrees F\n",
    "- $\\texttt{humidity}$: 83\n",
    "- $\\texttt{pizzas sold}$: 5500\n",
    "- $\\texttt{pressure}$: 850 millibar \n",
    "- $\\texttt{octopuses}$: 6\n",
    "- $\\texttt{Dan's shoe size}$: 9.5\n",
    "- $\\texttt{Tony's shoe size}$: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of Sharknado on this day: 57.19192599999951\n"
     ]
    }
   ],
   "source": [
    "prediction = 5 * 2.401187 + 47 * 0.318776 + 850 * 3.061834 + 1 * 1.255909 + 120 * 0.009931 + 70 * -0.468043 - 2542.04\n",
    "print(f\"Prediction of Sharknado on this day: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part G:** Consider the model you used in Part E, and consider the fact that you are trying to predict **sharknado hazard**. What is one critical drawback to the MLR model (or any MLR model) for predicting shardnado hazard? What are some modifications that could improve on this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Part G Conclusions:__\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p3'></a>\n",
    "\n",
    "### [35 points] Problem 3: FlipMaster5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file `flips.csv` you'll find the results of an experiment that was conducted with Stella O'Flaherty (the famous octopus data scientist) flipping coins. Her experiment was as follows. \n",
    "\n",
    "1. She reaches into her coin purse and grabs one of two coins, labeled $x$ and $y$. \n",
    "2. She flips her coin until it comes up heads 8 times, and records the coin ID and the number of flips it took to get 8 heads. \n",
    "3. She then replaces the coin in her coin purse and repeats the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A:**\n",
    "\n",
    "By considering the total number of flips and the total number of \"heads\" in the data file for each coin, estimate the bias of each coin $p_x$ and $p_y$, and use an appropriate statistical test to determine whether the coins have the same bias, i.e. whether $p_x$ and $p_y$ are the same. Perform your test at a significance level that will mistakenly reject the null hypothesis _when that null hypothesis is actually true_ 5% of the time. Report a p-value for your test, and clearly state your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Coin ID  Flips Required\n",
      "0       x               9\n",
      "1       x              13\n",
      "2       x              12\n",
      "3       y              12\n",
      "4       y              20\n"
     ]
    }
   ],
   "source": [
    "dfFlips = pd.read_csv('data/flips.csv', sep = '\\t')\n",
    "print(dfFlips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-val: 0.0\n"
     ]
    }
   ],
   "source": [
    "def bootstrapMeanDifference(M, dataList1, dataList2):\n",
    "    # 1: resample M times (the more the better) with replacement as many elements as the original \n",
    "    resamples1 = [np.random.choice(dataList1, replace = True, size = len(dataList1)) for i in range(M)]\n",
    "    # 2: compute the std of all of the elements of the resamples list (M elements in this new list)\n",
    "    means1 = [np.mean(one_resample) for one_resample in resamples1]\n",
    "    # 1: resample M times (the more the better) with replacement as many elements as the original \n",
    "    resamples2 = [np.random.choice(dataList2, replace = True, size = len(dataList2)) for i in range(M)]\n",
    "    # 2: compute the std of all of the elements of the resamples list (M elements in this new list)\n",
    "    means2 = [np.mean(one_resample) for one_resample in resamples2]\n",
    "    difference_means = np.mean(means1) - np.mean(means2)\n",
    "    bottom = np.sqrt((np.std(means1)**2/len(means1)) + (np.std(means2)**2/len(means2)))\n",
    "    z = difference_means / bottom\n",
    "    pval = stats.norm.cdf(z)\n",
    "    \n",
    "    # 3: compute the percentiles for these values (2.5%, and 97.5%)\n",
    "    return pval\n",
    "\n",
    "X = dfFlips.loc[dfFlips['Coin ID'] == 'x']['Flips Required']\n",
    "Y = dfFlips.loc[dfFlips['Coin ID'] == 'y']['Flips Required']\n",
    "p = bootstrapMeanDifference(10000, X, Y)\n",
    "print(f\"p-val: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Part A:__\n",
    "\n",
    "Hypothesis\n",
    "$$\n",
    "    H_0: \\mu_x = \\mu_y \\\\\n",
    "    H_1: \\mu_x \\neq \\mu_y\n",
    "$$\n",
    "\n",
    "\"mistakenly reject the null hypothesis when that null hypothesis is actually true 5% of the time\" means a significance level $\\alpha = 0.05$\n",
    "\n",
    "For the biases of the coins to be the same, we would expect the number of flips that it takes to get 8 heads would be the same for both coins.  To calculate this, we get a p-value from doing a two-sample testing for difference of means.  Because the data set is so small, we bootstrap the data and compute the Z statistic\n",
    "\n",
    "$$\n",
    "    Z = \\frac{(\\bar{x}_x - \\bar{x}_y)}{\\sqrt{\\frac{s_x^2}{m} + \\frac{s_y^2}{n}}} \\approx -154 \\\\\n",
    "    p = 0.0\n",
    "$$\n",
    "\n",
    "Because p-value is lower than the significance level 0.05 we conclude that the biases of the coins is NOT the same. Reject the null hypothesis.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B:** \n",
    "\n",
    "You learn that, actually, the coin $x$ is from a manufacturer that produces coins whose biases follow some statistical regularity. In particular, the bias of the $x$ coin is in the set $$p_x \\in \\{0.1, 0.2, 0.3, \\dots, 0.9\\}.$$ Furthermore, these biases all occur with equal probability. In other words, $\\tfrac{1}{8}$ of coins have bias $p_x=0.1$, $\\tfrac{1}{8}$ of coins have bias $p_x=0.2$, and so on. \n",
    "\n",
    "For each possible value of $p_x$, compute the probability that Stella's $x$ coin has bias of $p_x$, given the data in her data file. \n",
    "\n",
    "Plot your results with $p_x$ on the horizontal axis and $Pr(p_x \\mid \\text{data})$ on the vertical axis. Make the points or lines that you plot blue. Plots without axis labels will receive zero credit.\n",
    "\n",
    "_Hint_: We have done problems like this before! Think back to how you solved the problem on the midterm where you determined the probability that someone had ESP, given that they guessed the cards correctly. There was a \"rule\", and maybe a \"law\" involved in your calculation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C:**\n",
    "\n",
    "You learn that, actually, the coin $y$ is from a different manufacturer that produces coins whose biases follow some statistical regularity. In particular, the bias of the $y$ coin is in the set $$p_y \\in \\{0.1, 0.2, 0.3, \\dots, 0.9\\}.$$ Furthermore, these biases all occur with different probability. In particular, the probability that a coin has bias $p_y$ is proportional to $p_y$, which could be written as \n",
    "$$Pr(p_y) \\propto p_y \\quad \\text{for} \\quad p_y \\in \\{0.1, 0.2, 0.3, \\dots, 0.9\\}$$\n",
    "\n",
    "First, write clearly the PMF for $p_y$, based on the information above. \n",
    "\n",
    "Then, for each possible value of $p_y$, compute the probability that Stella's $y$ coin has bias of $p_y$, given the data in her data file. \n",
    "\n",
    "Plot your results with $p_y$ on the horizontal axis and $Pr(p_y \\mid \\text{data})$ on the vertical axis. Make the points or lines that you plot red. Plots without axis labels will receive zero credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D:**\n",
    "\n",
    "The information that you have about the manufacturer of coin $x$ and coin $y$ is called _prior information_ since it can influence the estimates of a coin's bias at which you arrive, given the data from the coin's flipping. We often call the distribution $Pr(p_x)$ or $Pr(p_y)$ a _prior distribution_, and call $Pr(p_x \\mid \\text{data})$ or $Pr(p_y \\mid \\text{data})$ a _posterior distribution_, since it represents the estimate that you arrive at after you have taken the data into account. \n",
    "\n",
    "You have already computed posterior distributions for each coin's bias. However, you'll now investigate the importance of the prior by _switching the priors for the two coins_.\n",
    "\n",
    "In other words, using the prior probabilities $Pr(p_x)$, what is your posterior distribution of $Pr(p_y \\mid \\text{data from y})$? Similarly, using the prior probabilities $Pr(p_y)$, what is your posterior distribution of $Pr(p_x \\mid \\text{data from x})$? \n",
    "\n",
    "Create two plots. \n",
    "\n",
    "1. In the first plot, show your results from Part B (the posterior distribution for $p_x$ with the correct prior) plotted with a blue solid line as well as your results from Part D for the posterior distribution for $p_x$ with the incorrect prior with a blue dashed line.  \n",
    "\n",
    "2. In the second plot, show your results from Part C (the posterior distribution for $p_y$ with the correct prior) with a red solid line as well as your results from Part D for the posterior distribution for $p_y$ with the incorrect prior with a red dashed line.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E:**\n",
    "\n",
    "What is the name of the distribution that Stella's experiment is drawn from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to Problem 2](#p2)\n",
    "\n",
    "<a id='footnote'></a> Yeah yeah - fresh water versus salt water - I know, I know. But sharknadoes also are not real, so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
